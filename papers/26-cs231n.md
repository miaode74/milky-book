# è®ºæ–‡è§£è¯»ä¸Žå¤çŽ°ï¼šCS231n - Convolutional Neural Networks for Visual Recognition

## 1. ä¸€å¥è¯æ¦‚è¿°
è¿™æ˜¯ä¸€ä»½è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„â€œåŽŸæœ¬çœŸç»â€ï¼ˆThe Vision Bibleï¼‰ï¼Œå®ƒç³»ç»Ÿæ€§åœ°æž„å»ºäº†ä»Žåƒç´ åˆ°è¯­ä¹‰é¢„æµ‹çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ï¼Œç¡®ç«‹äº†ä»¥å·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰ä¸ºæ ¸å¿ƒçš„ç«¯åˆ°ç«¯å›¾åƒåˆ†ç±»èŒƒå¼ï¼Œå¹¶å°†â€œæ•°æ®é©±åŠ¨â€ä¸Žâ€œåå‘ä¼ æ’­â€ç¡®ç«‹ä¸ºçŽ°ä»£è§†è§‰ç³»ç»Ÿçš„åŸºçŸ³ã€‚

## 2. Abstract: è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿæœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ
**CS231n** å¹¶éžå•ç¯‡è®ºæ–‡ï¼Œè€Œæ˜¯æ–¯å¦ç¦å¤§å­¦çš„ä¸€é—¨ç»å…¸è¯¾ç¨‹ï¼Œå…¶è®²ä¹‰ä¸Žä½œä¸šæž„æˆäº†çŽ°ä»£æ·±åº¦å­¦ä¹ è§†è§‰ç ”ç©¶çš„å…¥é—¨æ ‡å‡†ã€‚å®ƒè¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ **Visual Recognitionï¼ˆè§†è§‰è¯†åˆ«ï¼‰** çš„è‡ªåŠ¨åŒ–ä¸Žé€šç”¨åŒ–ã€‚

åœ¨æ­¤ä¹‹å‰ï¼Œè®¡ç®—æœºè§†è§‰é«˜åº¦ä¾èµ–äººå·¥è®¾è®¡çš„ç‰¹å¾ï¼ˆå¦‚ SIFT, HOGï¼‰ã€‚CS231n çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºŽï¼š
1.  **èŒƒå¼è½¬ç§»**ï¼šå®Œæ•´é˜è¿°äº†ä»Žâ€œäººå·¥ç‰¹å¾å·¥ç¨‹â€å‘â€œç«¯åˆ°ç«¯ç‰¹å¾å­¦ä¹ â€çš„è½¬ç§»ï¼Œç³»ç»Ÿå±•ç¤ºäº†ç›´æŽ¥ä»ŽåŽŸå§‹åƒç´ å­¦ä¹ å±‚çº§åŒ–ç‰¹å¾çš„å¯è¡Œæ€§ã€‚
2.  **æ–¹æ³•è®ºä½“ç³»åŒ–**ï¼šå°†ç¥žç»ç½‘ç»œçš„è®­ç»ƒè§£æž„ä¸ºè¯„åˆ†å‡½æ•°ï¼ˆScore Functionï¼‰ã€æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰å’Œä¼˜åŒ–ç®—æ³•ï¼ˆOptimizationï¼‰ä¸‰ä¸ªæ­£äº¤ç»„ä»¶ï¼Œä¸ºåŽæ¥çš„æ·±åº¦å­¦ä¹ æ¡†æž¶è®¾è®¡å¥ å®šäº†é€»è¾‘åŸºç¡€ã€‚
3.  **CNN æ™®åŠ**ï¼šæ·±å…¥å‰–æžäº†å·ç§¯ï¼ˆConvolutionï¼‰ä¸Žæ± åŒ–ï¼ˆPoolingï¼‰çš„æ•°å­¦åŽŸç†ï¼Œè§£é‡Šäº†å‚æ•°å…±äº«ï¼ˆParameter Sharingï¼‰å¦‚ä½•åˆ©ç”¨å›¾åƒçš„ç©ºé—´ç»“æž„ï¼ˆSpatial Structureï¼‰ï¼Œæžå¤§åœ°æŽ¨åŠ¨äº† CNN åœ¨ AlexNet ä¹‹åŽçš„çˆ†å‘å¼åº”ç”¨ã€‚

## 3. Introduction: è®ºæ–‡çš„åŠ¨æœºæ˜¯ä»€ä¹ˆï¼Ÿè¯·ä»”ç»†æ¢³ç†æ•´ä¸ªæ•…äº‹é€»è¾‘
è¿™ä»½ææ–™çš„é€»è¾‘æž„å»ºæžå…¶ä¸¥å¯†ï¼Œå®ƒè®²è¿°äº†ä¸€ä¸ªä»Žâ€œç›´è§‰â€åˆ°â€œæ•°å­¦æŠ½è±¡â€å†åˆ°â€œä»¿ç”Ÿæž¶æž„â€çš„æ•…äº‹ã€‚

### 3.1 æŒ‘æˆ˜ï¼šè¯­ä¹‰é¸¿æ²Ÿ
è®¡ç®—æœºçœ‹åˆ°çš„åªæ˜¯ä¸€ä¸ª $[0, 255]$ çš„å·¨å¤§æ•°å­—çŸ©é˜µï¼Œè€Œäººç±»çœ‹åˆ°çš„æ˜¯â€œçŒ«â€æˆ–â€œç‹—â€ã€‚è¿™ç§åƒç´ å€¼ä¸Žå…¶è¯­ä¹‰å†…å®¹ä¹‹é—´çš„å·¨å¤§å·®å¼‚è¢«ç§°ä¸º **Semantic Gap**ã€‚ä¼ ç»Ÿçš„åŸºäºŽè§„åˆ™çš„æ–¹æ³•ï¼ˆå¦‚è¾¹ç¼˜æ£€æµ‹ï¼‰æ— æ³•åº”å¯¹è§†è§’å˜åŒ–ã€å…‰ç…§ã€é®æŒ¡å’Œå½¢å˜ã€‚

### 3.2 èµ·ç‚¹ï¼šæ•°æ®é©±åŠ¨æ–¹æ³•ï¼ˆData-Driven Approachï¼‰
æ—¢ç„¶æ— æ³•ç¡¬ç¼–ç è§„åˆ™ï¼Œä¸å¦‚è®©æœºå™¨é€šè¿‡æ•°æ®åŽ»â€œçœ‹â€ã€‚
> The computer is given a set of images... and effectively summarizes the knowledge into a model.
è¿™å°±å¼•å‡ºäº† **k-Nearest Neighbors (kNN)**ã€‚è™½ç„¶ kNN é€»è¾‘ç®€å•ï¼ˆâ€œæˆ‘çœ‹ä½ åƒè°ï¼Œä½ å°±æ˜¯è°â€ï¼‰ï¼Œä½†å®ƒåœ¨æµ‹è¯•æ—¶éœ€è¦éåŽ†æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œè®¡ç®—æˆæœ¬æžé«˜ä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼ˆMemorization vs Learningï¼‰ã€‚

### 3.3 è¿›åŒ–ï¼šçº¿æ€§åˆ†ç±»å™¨ï¼ˆParametric Approachï¼‰
ä¸ºäº†è§£å†³ kNN çš„ä½Žæ•ˆï¼Œå¼•å…¥äº†å‚æ•°åŒ–æ¨¡åž‹ $f(x, W) = Wx + b$ã€‚
> One template per class... The weights $W$ correspond to class templates.
è¿™é‡Œï¼Œ$W$ è¢«è§£é‡Šä¸ºæ¯ä¸ªç±»åˆ«çš„â€œå¹³å‡æ¨¡æ¿â€ã€‚é€šè¿‡ **SVMï¼ˆHinge Lossï¼‰** æˆ– **Softmaxï¼ˆCross-Entropy Lossï¼‰** æ¥è¡¡é‡é¢„æµ‹çš„å¥½åï¼Œå¹¶é€šè¿‡ **SGD** è¿›è¡Œä¼˜åŒ–ã€‚è¿™æ˜¯æœºå™¨å­¦ä¹ çš„ç»å…¸å½¢æ€ã€‚

### 3.4 è´¨å˜ï¼šç¥žç»ç½‘ç»œä¸Ž CNN
çº¿æ€§æ¨¡åž‹åªèƒ½é€šè¿‡â€œç”»ä¸€æ¡ç›´çº¿â€æ¥åˆ†ç±»ï¼Œæ— æ³•å¤„ç†å¼‚æˆ–ï¼ˆXORï¼‰ç­‰éžçº¿æ€§é—®é¢˜ã€‚
å¼•å…¥ **Hidden Layer** å’Œ **Non-linearity (ReLU)** åŽï¼Œæ¨¡åž‹å…·å¤‡äº†æ‹Ÿåˆå¤æ‚æµå½¢çš„èƒ½åŠ›ã€‚è¿›ä¸€æ­¥åœ°ï¼Œé’ˆå¯¹å›¾åƒæ•°æ®çš„**ç©ºé—´å±€éƒ¨æ€§ï¼ˆLocal Connectivityï¼‰**ï¼Œå…¨è¿žæŽ¥å±‚ï¼ˆFully Connectedï¼‰ç”±äºŽå‚æ•°é‡è¿‡å¤§ä¸”å¿½ç•¥ç©ºé—´ç»“æž„è€Œè¢« **å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰** å–ä»£ã€‚è¿™æœ€ç»ˆå¯¼å‘äº†çŽ°ä»£è§†è§‰çš„åŸºçŸ³æž¶æž„ï¼š**ConvNet**ã€‚

## 4. Method: è§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿè¯·æ¢³ç†æ­¥éª¤ã€å…¬å¼ã€ç­–ç•¥

è¯¥æ–¹æ³•è®ºé€šè¿‡ä»¥ä¸‹äº”ä¸ªæ ¸å¿ƒæ¨¡å—æž„å»ºäº†çŽ°ä»£è§†è§‰ç³»ç»Ÿï¼š

### 4.1 è¯„åˆ†å‡½æ•°ä¸Žå…¨è¿žæŽ¥ç½‘ç»œ
å¯¹äºŽå…¨è¿žæŽ¥ç½‘ç»œï¼ˆMLPï¼‰ï¼Œè¾“å…¥å›¾åƒè¢«æ‹‰å¹³ä¸ºå‘é‡ $x$ã€‚ä¸€ä¸ªä¸¤å±‚ç½‘ç»œçš„è¯„åˆ†å‡½æ•°å®šä¹‰ä¸ºï¼š
$$f(x) = W_2 \cdot \max(0, W_1 x + b_1) + b_2$$
å…¶ä¸­ $\max(0, \cdot)$ æ˜¯ **ReLU (Rectified Linear Unit)** æ¿€æ´»å‡½æ•°ï¼Œå®ƒè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¹¶åŠ é€Ÿæ”¶æ•›ã€‚

### 4.2 æŸå¤±å‡½æ•°ï¼ˆLoss Functionsï¼‰
ä¸ºäº†é‡åŒ–â€œé¢„æµ‹æœ‰å¤šå·®â€ï¼Œè®ºæ–‡ï¼ˆè¯¾ç¨‹ï¼‰å¯¹æ¯”äº†ä¸¤ç§æŸå¤±ï¼š
1.  **Multiclass SVM Loss**: å¼ºè°ƒè¾¹ç¼˜é—´éš”ï¼ˆMarginï¼‰ã€‚
    $$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)$$
2.  **Softmax Loss**: å¼ºè°ƒæ¦‚çŽ‡åˆ†å¸ƒçš„æ‹Ÿåˆã€‚
    $$L_i = -\log\left(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}}\right)$$
    > Softmax classifier interprets the scores as unnormalized log probabilities.
    è¿™ä¸ä»…ç»™å‡ºäº†åˆ†ç±»ç»“æžœï¼Œè¿˜ç»™å‡ºäº†ç½®ä¿¡åº¦ã€‚

### 4.3 å·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ ¸å¿ƒç®—å­
è¿™æ˜¯å¤„ç†è§†è§‰æ•°æ®çš„æ ¸å¿ƒåˆ›æ–°ã€‚ä¸Žå…¨è¿žæŽ¥å±‚ä¸åŒï¼ŒCNN å‡è®¾è¾“å…¥å…·æœ‰ç½‘æ ¼ç»“æž„ã€‚

* **å·ç§¯ï¼ˆConvolutionï¼‰**ï¼š
    è¾“å‡ºå°ºå¯¸çš„è®¡ç®—å…¬å¼è‡³å…³é‡è¦ï¼š
    $$Output = \frac{W - F + 2P}{S} + 1$$
    å…¶ä¸­ $W$ æ˜¯è¾“å…¥å°ºå¯¸ï¼Œ$F$ æ˜¯å·ç§¯æ ¸å¤§å°ï¼Œ$P$ æ˜¯å¡«å……ï¼Œ$S$ æ˜¯æ­¥é•¿ã€‚è¿™ä¸€å…¬å¼å†³å®šäº†ç‰¹å¾å›¾åœ¨ç½‘ç»œä¸­çš„æµåŠ¨å½¢æ€ã€‚
    > Parameter sharing... assumes that if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2).
    è¿™ç§**å¹³ç§»ä¸å˜æ€§**å¤§å¤§å‡å°‘äº†å‚æ•°é‡ã€‚

* **æ± åŒ–ï¼ˆPoolingï¼‰**ï¼š
    é€šå¸¸ä½¿ç”¨ Max Poolingï¼Œç”¨äºŽé€šè¿‡é™é‡‡æ ·å‡å°‘å‚æ•°å¹¶èŽ·å¾—ç©ºé—´ä¸å˜æ€§ã€‚

### 4.4 ä¼˜åŒ–ï¼ˆOptimizationï¼‰
ä»…ä»…å®šä¹‰ Loss æ˜¯ä¸å¤Ÿçš„ï¼Œå¿…é¡»é€šè¿‡**åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**è®¡ç®—æ¢¯åº¦ $\nabla_W L$ã€‚
* **SGD**: $W \leftarrow W - \eta \nabla_W L$
* **Momentum**: å¼•å…¥ç‰©ç†ä¸­çš„â€œåŠ¨é‡â€æ¦‚å¿µï¼Œå¸®åŠ©ç©¿è¶Šå¹³å¦åŒºåŸŸå’Œå±€éƒ¨æžå°å€¼ã€‚
    $$v_{t+1} = \rho v_t + \nabla L, \quad W_{t+1} = W_t - \alpha v_{t+1}$$

### 4.5 è®­ç»ƒæŠ€å·§ï¼ˆBabysittingï¼‰
CS231n å¼ºè°ƒå·¥ç¨‹å®žè·µï¼š
* **åˆå§‹åŒ–**ï¼šXavier æˆ– He åˆå§‹åŒ–ï¼ˆé˜²æ­¢æ¿€æ´»å€¼åœ¨å‰å‘ä¼ æ’­ä¸­æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼‰ã€‚
* **é¢„å¤„ç†**ï¼šé›¶å‡å€¼åŒ–ï¼ˆZero-centeringï¼‰ã€‚
* **æ­£åˆ™åŒ–**ï¼šL2 Weight Decay æˆ– Dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

```mermaid
graph LR
    subgraph Data Processing
    A[Raw Image 32x32x3] --> B[Zero-Center Norm]
    end

    subgraph Architecture
    B --> C[Conv Layer 5x5]
    C --> D[ReLU Activation]
    D --> E[Max Pool 2x2]
    E --> F[FC Layer]
    F --> G[Softmax Scores]
    end

    subgraph Learning
    G --> H{Loss Function}
    H -->|Calculate Loss| I[Cross Entropy]
    I -->|Backprop| J[Compute Gradients]
    J -->|SGD/Adam| K[Update Weights]
    K -.-> C
    K -.-> F
    end

```

## 5. Experiment: ä¸»å®žéªŒä¸Žåˆ†æžå®žéªŒåˆ†åˆ«åšäº†ä»€ä¹ˆï¼Ÿç»“æžœå¦‚ä½•ï¼Ÿ

è™½ç„¶ CS231n æ˜¯è¯¾ç¨‹ï¼Œä½†å…¶ä½œä¸šè®¾ç½®å¤çŽ°äº†ä»Žçº¿æ€§æ¨¡åž‹åˆ° CNN çš„æ€§èƒ½è·ƒå‡ï¼Œé€šå¸¸åŸºäºŽ **CIFAR-10** æ•°æ®é›†ã€‚

### 5.1 ä¸»å®žéªŒï¼šåˆ†ç±»å‡†ç¡®çŽ‡å¯¹æ¯”

å®žéªŒåœ¨ CIFAR-10ï¼ˆ10ç±»ï¼Œ32x32 RGBå›¾åƒï¼‰ä¸Šå¯¹æ¯”äº†ä¸åŒæ¨¡åž‹çš„è¡¨çŽ°ã€‚

* **kNN**: ä½œä¸º Baselineï¼Œåœ¨åƒç´ ç©ºé—´é€šè¿‡ L2 è·ç¦»è®¡ç®—ã€‚å‡†ç¡®çŽ‡é€šå¸¸è¾ƒä½Žï¼ˆ~20-30%ï¼‰ï¼Œä¸”å—èƒŒæ™¯é¢œè‰²å½±å“ä¸¥é‡ã€‚
* **Linear Classifier (Softmax)**: å‡†ç¡®çŽ‡æå‡è‡³ ~30-40%ã€‚
> Weights look like averaged class templates.
> å¯è§†åŒ–å‘çŽ°ï¼Œçº¿æ€§åˆ†ç±»å™¨å­¦ä¹ åˆ°çš„æ˜¯æ¨¡ç³Šçš„å¹³å‡å›¾åƒï¼ˆä¾‹å¦‚ï¼Œâ€œé©¬â€çš„æ¨¡æ¿çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªåŒå¤´çš„é©¬ï¼Œå› ä¸ºè®­ç»ƒé›†é‡Œæœ‰å‘å·¦å’Œå‘å³çš„é©¬ï¼‰ã€‚


* **Two-Layer NN**: å¼•å…¥ä¸€ä¸ªéšè—å±‚ï¼Œå‡†ç¡®çŽ‡æå‡è‡³ ~50%ã€‚æ¨¡åž‹å¼€å§‹èƒ½å¤Ÿå¤„ç†çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®ã€‚
* **ConvNet (Mini AlexNet)**: ä½¿ç”¨ Conv-ReLU-Pool ç»“æž„ï¼Œå‡†ç¡®çŽ‡è½»æ¾çªç ´ 60-70%ï¼ˆç”šè‡³æ›´é«˜ï¼Œå–å†³äºŽå±‚æ•°å’Œè®­ç»ƒæ—¶é•¿ï¼‰ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ å¨åŠ›çš„ç›´æŽ¥ä½“çŽ°ã€‚

### 5.2 åˆ†æžå®žéªŒï¼šå¯è§†åŒ–ä¸Žè§£é‡Šæ€§

è¿™æ˜¯ CS231n æžå…·æ•™å­¦æ„ä¹‰çš„éƒ¨åˆ†ã€‚

* **æ»¤æ³¢å™¨å¯è§†åŒ–**ï¼š
ç¬¬ä¸€å±‚å·ç§¯æ ¸ï¼ˆFiltersï¼‰å¯è§†åŒ–åŽå‘ˆçŽ°å‡ºç±»ä¼¼ Gabor æ»¤æ³¢å™¨çš„å½¢æ€â€”â€”**è¾¹ç¼˜ï¼ˆEdgesï¼‰å’Œåè‰²ï¼ˆOpponent Colorsï¼‰**ã€‚è¿™è¯æ˜Žäº† CNN æ¨¡ä»¿äº†äººç±»è§†è§‰çš®å±‚ V1 åŒºçš„è¿ä½œæœºåˆ¶ã€‚
* **Saliency Maps**ï¼š
é€šè¿‡è®¡ç®— ï¼Œå¯ä»¥é«˜äº®å‡ºå›¾åƒä¸­å“ªäº›åƒç´ å¯¹åˆ†ç±»ç»“æžœè´¡çŒ®æœ€å¤§ã€‚å®žéªŒè¡¨æ˜Žï¼Œç½‘ç»œå…³æ³¨çš„æ˜¯ç‰©ä½“çš„ä¸»ä½“è½®å»“ï¼Œè€ŒéžèƒŒæ™¯å™ªå£°ã€‚

### 5.3 æ¶ˆèžå®žéªŒï¼šè¶…å‚æ•°æ•æ„Ÿæ€§

* **å­¦ä¹ çŽ‡ï¼ˆLearning Rateï¼‰**ï¼šå®žéªŒå±•ç¤ºäº†å­¦ä¹ çŽ‡è¿‡å¤§å¯¼è‡´ Loss çˆ†ç‚¸ï¼ˆNaNï¼‰ï¼Œè¿‡å°å¯¼è‡´æ”¶æ•›åœæ»žã€‚
* **æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰**ï¼šå±•ç¤ºäº† L2 æƒ©ç½šé¡¹å¦‚ä½•å¹³æ»‘æƒé‡åˆ†å¸ƒï¼Œé˜²æ­¢æ¨¡åž‹æ­»è®°ç¡¬èƒŒè®­ç»ƒæ ·æœ¬ã€‚

## 6. Numpy ä¸Ž Torch å¯¹ç…§å®žçŽ°ï¼ˆå« code-groupï¼‰

### å®žçŽ°è¯´æ˜Ž

è¿™ä»½ä»£ç å®Œæ•´å®žçŽ°äº† CS231n è¯¾ç¨‹çš„æ ¸å¿ƒ Pipelineï¼Œæ¶µç›–äº†ä»Žæ•°æ®ç”Ÿæˆã€kNNã€çº¿æ€§åˆ†ç±»å™¨ã€ä¼˜åŒ–å™¨åˆ° CNN çš„å…¨éƒ¨å†…å®¹ã€‚

**ä»£ç å¯¹åº”å…³ç³»ä¸Žå¼ é‡è¯´æ˜Žï¼š**

1. **Data**: `generate_synthetic_cifar` å¯¹åº”æ•°æ®åŠ è½½ã€‚è¾“å…¥å½¢çŠ¶ `(N, 32, 32, 3)`ï¼Œä¸ºäº†é€‚åº”å…¨è¿žæŽ¥å±‚éœ€ Flatten ä¸º `(N, 3072)`ã€‚
2. **kNN**: `KNearestNeighbor` å±•ç¤ºäº†éžå‚æ•°æ–¹æ³•ã€‚
3. **Linear**: `LinearClassifier` å®žçŽ°  åŠ Softmax/SVM Lossã€‚å¼ é‡å½¢çŠ¶ï¼šInput `(N, D)`, Weight `(D, C)`ã€‚
4. **NN/CNN**: `TwoLayerNet` å’Œ `SimpleCNN`ã€‚
* **Numpy**: å¿…é¡»æ‰‹åŠ¨æŽ¨å¯¼åå‘ä¼ æ’­æ¢¯åº¦ï¼ˆä¾‹å¦‚ `dscores`ï¼‰ï¼Œä½¿ç”¨ `im2col` æˆ–å¤šé‡å¾ªçŽ¯ï¼ˆä»£ç ä¸­ä½¿ç”¨å¾ªçŽ¯ï¼‰å®žçŽ°å·ç§¯ã€‚**å‡è®¾**ï¼šè¾“å…¥æ•°æ®æœªå½’ä¸€åŒ–ï¼ˆä»£ç ä¸­æœ‰ç®€å•çš„ `* 0.1` å™ªå£°ï¼‰ï¼Œä½†åœ¨è®­ç»ƒæ—¶é€šå¸¸éœ€è¦é¢„å¤„ç†ã€‚
* **Torch**: åˆ©ç”¨ `autograd` è‡ªåŠ¨æ±‚å¯¼ï¼Œåˆ©ç”¨ `nn.Conv2d` ç­‰é«˜åº¦ä¼˜åŒ–çš„ç®—å­ã€‚



**ä¸»è¦å·®å¼‚ç‚¹ï¼š**

* **Backprop**: Numpy ç‰ˆæœ¬éœ€è¦æ‰‹åŠ¨å†™å‡º `dW = X.T @ dscores` ç­‰åå‘ä¼ æ’­å…¬å¼ï¼›Torch ç‰ˆæœ¬åªéœ€ `loss.backward()`ã€‚
* **ç»´åº¦é¡ºåº**: Numpy ä»£ç å¤„ç†å›¾åƒæ—¶å¤šå¤„ä½¿ç”¨äº† `(N, H, W, C)` æ ¼å¼ï¼ˆè¿™æ˜¯ matplotlib å‹å¥½æ ¼å¼ï¼‰ï¼Œä½†åœ¨å·ç§¯å±‚è½¬æ¢ä¸ºäº† `(N, C, H, W)`ã€‚PyTorch åŽŸç”Ÿå¼ºåˆ¶è¦æ±‚ `(N, C, H, W)`ã€‚
* **å·ç§¯å®žçŽ°**: Numpy ç‰ˆä¸ºäº†æ•™å­¦ä½¿ç”¨äº†åµŒå¥—å¾ªçŽ¯ï¼ˆæžæ…¢ï¼‰ï¼›Torch ç‰ˆä½¿ç”¨äº†åº•å±‚çš„ cuDNN/BLAS ä¼˜åŒ–ï¼ˆæžå¿«ï¼‰ã€‚

::: code-group

```python [Numpy]
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import convolve
from typing import Tuple, List, Dict
from dataclasses import dataclass

np.random.seed(42)
plt.style.use('seaborn-v0_8-darkgrid')

print("CS231n: From Pixels to Predictions")
print("NumPy version:", np.__version__)
print("\nReady to learn computer vision!")

# Section 1: Dataset - Synthetic CIFAR-10
# CS231n uses CIFAR-10 (10 classes, 32Ã—32 RGB images). We'll generate synthetic data with similar structure.
# Data Generation Strategy
# Create procedural 32Ã—32 images with class-specific patterns:
# Class 0-2: Spirals (different rotations)
# Class 3-5: Checkerboards (different frequencies)
# Class 6-7: Gradients (different directions)
# Class 8-9: Circles (different sizes)
# This gives us:
# Learnable patterns (not pure noise)
# Visual diversity (test different features)
# Instant generation (no downloads)

def generate_synthetic_cifar(num_samples: int = 1000, 
                             img_size: int = 32, 
                             num_classes: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Generate synthetic CIFAR-10 style dataset.
        Returns:
        X: (N, 32, 32, 3) RGB images
        y: (N,) class labels
    """
    X = np.zeros((num_samples, img_size, img_size, 3))
    y = np.random.randint(0, num_classes, num_samples)
    
    for i in range(num_samples):
        label = y[i]
        img = np.random.randn(img_size, img_size, 3) * 0.1  # Base noise
        
        # Class-specific patterns
        if label < 3:  # Spirals
            theta = np.linspace(0, 4*np.pi, 200)
            r = np.linspace(0, img_size/2, 200)
            rotation = label * np.pi / 3
            x_coords = (r * np.cos(theta + rotation) + img_size/2).astype(int)
            y_coords = (r * np.sin(theta + rotation) + img_size/2).astype(int)
            valid = (x_coords >= 0) & (x_coords < img_size) & (y_coords >= 0) & (y_coords < img_size)
            img[y_coords[valid], x_coords[valid], :] = [1.0, 0.5, 0.0]
            
        elif label < 6:  # Checkerboards
            freq = (label - 2) * 2
            xx, yy = np.meshgrid(np.arange(img_size), np.arange(img_size))
            pattern = ((xx // freq) + (yy // freq)) % 2
            img[:, :, 0] = pattern
            img[:, :, 1] = 1 - pattern
            
        elif label < 8:  # Gradients
            if label == 6:
                img[:, :, 0] = np.linspace(0, 1, img_size)[None, :]
            else:
                img[:, :, 1] = np.linspace(0, 1, img_size)[:, None]
                
        else:  # Circles
            radius = (label - 7) * 8 + 5
            yy, xx = np.ogrid[:img_size, :img_size]
            circle = ((xx - img_size/2)**2 + (yy - img_size/2)**2 <= radius**2)
            img[circle, 2] = 1.0
        
        X[i] = np.clip(img, 0, 1)
    
    return X, y

# Generate train/val/test splits
print("Generating synthetic CIFAR-10...\n")
X_train, y_train = generate_synthetic_cifar(num_samples=2000)
X_val, y_val = generate_synthetic_cifar(num_samples=400)
X_test, y_test = generate_synthetic_cifar(num_samples=400)

print(f"Training set:   X={X_train.shape}, y={y_train.shape}")
print(f"Validation set: X={X_val.shape}, y={y_val.shape}")
print(f"Test set:       X={X_test.shape}, y={y_test.shape}")

# Visualize samples
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
for i in range(10):
    ax = axes[i // 5, i % 5]
    idx = np.where(y_train == i)[0][0]
    ax.imshow(X_train[idx])
    ax.set_title(f'Class {i}')
    ax.axis('off')
plt.suptitle('Synthetic CIFAR-10: Sample Images per Class', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Flatten for traditional classifiers
X_train_flat = X_train.reshape(len(X_train), -1)  # (N, 3072)
X_val_flat = X_val.reshape(len(X_val), -1)
X_test_flat = X_test.reshape(len(X_test), -1)

print(f"\nFlattened shape: {X_train_flat.shape} (32Ã—32Ã—3 = 3072 pixels)")
print("\nâœ“ Dataset ready!")

# Section 2: k-Nearest Neighbors (kNN)
# The simplest classifier: Given test image, find k closest training images and vote on label.
# Algorithm
# Compute distance to all training images: 
# Find k nearest neighbors
# Majority vote on their labels
# Distance Metrics
# L1 (Manhattan):
# L2 (Euclidean):

# Why kNN Matters
# No training: Just memorize data
# Test-time slow: O(N) per prediction
# Baseline: Establishes lower bound
# Never used in practice: But important pedagogically!

class KNearestNeighbor:
    """k-Nearest Neighbor classifier."""
    
    def __init__(self, k: int = 5):
        self.k = k
        self.X_train = None
        self.y_train = None
    
    def train(self, X: np.ndarray, y: np.ndarray):
        """'Train' by memorizing data (no actual training!)."""
        self.X_train = X
        self.y_train = y
        print(f"kNN 'trained' on {len(X)} samples")
    
    def predict(self, X: np.ndarray, distance_metric: str = 'l2') -> np.ndarray:
        """Predict labels for test data.
                Args:
            X: (N_test, D) test data
            distance_metric: 'l1' or 'l2'
                Returns:
            y_pred: (N_test,) predicted labels
        """
        num_test = X.shape[0]
        y_pred = np.zeros(num_test, dtype=int)
        
        for i in range(num_test):
            # Compute distances to all training samples
            if distance_metric == 'l1':
                distances = np.sum(np.abs(self.X_train - X[i]), axis=1)
            else:  # l2
                distances = np.sqrt(np.sum((self.X_train - X[i])**2, axis=1))
            
            # Find k nearest neighbors
            k_nearest = np.argsort(distances)[:self.k]
            k_nearest_labels = self.y_train[k_nearest]
            
            # Majority vote
            y_pred[i] = np.argmax(np.bincount(k_nearest_labels))
        
        return y_pred
    
    def compute_accuracy(self, X: np.ndarray, y: np.ndarray, **kwargs) -> float:
        """Compute classification accuracy."""
        y_pred = self.predict(X, **kwargs)
        return np.mean(y_pred == y)

# Train kNN (just memorize)
print("Testing k-Nearest Neighbors...\n")
knn = KNearestNeighbor(k=5)
knn.train(X_train_flat, y_train)

# Test different k values
k_values = [1, 3, 5, 10, 20]
accuracies_l1 = []
accuracies_l2 = []

print("\nTesting different k values...")
for k in k_values:
    knn.k = k
    acc_l1 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l1')
    acc_l2 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l2')
    accuracies_l1.append(acc_l1)
    accuracies_l2.append(acc_l2)
    print(f"  k={k:2d}: L1={acc_l1:.1%}, L2={acc_l2:.1%}")

# Plot accuracy vs k
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Accuracy vs k
axes[0].plot(k_values, accuracies_l1, 'o-', linewidth=2, markersize=8, label='L1 distance')
axes[0].plot(k_values, accuracies_l2, 's-', linewidth=2, markersize=8, label='L2 distance')
axes[0].set_xlabel('k (number of neighbors)', fontsize=11)
axes[0].set_ylabel('Validation Accuracy', fontsize=11)
axes[0].set_title('kNN: Hyperparameter Tuning', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Confusion matrix for k=5
knn.k = 5
y_pred = knn.predict(X_val_flat[:200], distance_metric='l2')
y_true = y_val[:200]
confusion = np.zeros((10, 10))
for true, pred in zip(y_true, y_pred):
    confusion[true, pred] += 1

im = axes[1].imshow(confusion, cmap='Blues')
axes[1].set_xlabel('Predicted Label', fontsize=11)
axes[1].set_ylabel('True Label', fontsize=11)
axes[1].set_title('Confusion Matrix (k=5, L2)', fontsize=12, fontweight='bold')
plt.colorbar(im, ax=axes[1])
plt.tight_layout()
plt.show()

print("\nðŸ”‘ Key insights:")
print("   â€¢ kNN: No training, slow at test time")
print("   â€¢ k=1: Overfits (memorizes noise)")
print("   â€¢ k too large: Underfits (averages too much)")
print("   â€¢ Best k: Found via validation set")
print(f"   â€¢ Best accuracy: {max(max(accuracies_l1), max(accuracies_l2)):.1%} (baseline!)")
print("\nâœ“ kNN complete! Let's do better with parametric models...")

# Section 3: Linear Classifiers - SVM and Softmax
# Parametric models: Learn weight matrix to predict scores.
# Score Function
# where:
# : Input image (3072 pixels)
# : Weight matrix (10 Ã— 3072)
# : Bias vector (10,)
# Output: 
# : Class scores (10,)
# Loss Functions
# 1. Multiclass SVM Loss (Hinge Loss)
# where is the margin.
# Intuition: Correct class score should be at least higher than wrong classes.
# 2. Softmax Loss (Cross-Entropy)
# Intuition: Maximize log-probability of correct class.
# Regularization
# Add penalty to prevent overfitting:
# Common choices:
# L2: 
# (weight decay)
# L1: 
# (sparsity)

class LinearClassifier:
    """Linear classifier with SVM or Softmax loss."""
    
    def __init__(self, input_dim: int = 3072, num_classes: int = 10):
        self.W = np.random.randn(input_dim, num_classes) * 0.0001
        self.b = np.zeros(num_classes)
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """Compute class scores.
                Args:
            X: (N, D) input data
                Returns:
            scores: (N, C) class scores
        """
        return X @ self.W + self.b
    
    def svm_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:
        """Compute SVM loss and gradients.
                Returns:
            loss: Scalar loss
            dW: Gradient of loss w.r.t. W
            db: Gradient of loss w.r.t. b
        """
        N = X.shape[0]
        scores = self.forward(X)  # (N, C)
        
        # Compute margins
        correct_scores = scores[range(N), y].reshape(-1, 1)  # (N, 1)
        margins = np.maximum(0, scores - correct_scores + 1)  # (N, C)
        margins[range(N), y] = 0  # Don't count correct class
        
        # Loss
        loss = np.sum(margins) / N
        loss += reg * np.sum(self.W ** 2)  # L2 regularization
        
        # Gradients
        binary = (margins > 0).astype(float)  # (N, C)
        binary[range(N), y] = -np.sum(binary, axis=1)  # Correct class gets negative
        
        dW = (X.T @ binary) / N + 2 * reg * self.W
        db = np.sum(binary, axis=0) / N
        
        return loss, dW, db
    
    def softmax_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:
        """Compute Softmax loss and gradients.
                Returns:
            loss: Scalar loss
            dW: Gradient of loss w.r.t. W
            db: Gradient of loss w.r.t. b
        """
        N = X.shape[0]
        scores = self.forward(X)  # (N, C)
        
        # Numerical stability: shift scores
        scores -= np.max(scores, axis=1, keepdims=True)
        
        # Softmax probabilities
        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # (N, C)
        
        # Loss
        correct_log_probs = -np.log(probs[range(N), y] + 1e-10)
        loss = np.sum(correct_log_probs) / N
        loss += reg * np.sum(self.W ** 2)
        
        # Gradients
        dscores = probs.copy()
        dscores[range(N), y] -= 1  # Subtract 1 from correct class
        dscores /= N
        
        dW = X.T @ dscores + 2 * reg * self.W
        db = np.sum(dscores, axis=0)
        
        return loss, dW, db
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predict class labels."""
        scores = self.forward(X)
        return np.argmax(scores, axis=1)
    
    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:
        """Compute classification accuracy."""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

def train_linear_classifier(classifier: LinearClassifier,
                           X_train: np.ndarray,
                           y_train: np.ndarray,
                           X_val: np.ndarray,
                           y_val: np.ndarray,
                           loss_function: str = 'softmax',
                           learning_rate: float = 1e-3,
                           reg: float = 1e-5,
                           num_iters: int = 1000,
                           batch_size: int = 200,
                           verbose: bool = True) -> Dict:
    """Train linear classifier using SGD.
        Returns:
        Dictionary with training history
    """
    N = X_train.shape[0]
    loss_history = []
    train_acc_history = []
    val_acc_history = []
    
    for it in range(num_iters):
        # Sample mini-batch
        batch_indices = np.random.choice(N, batch_size, replace=False)
        X_batch = X_train[batch_indices]
        y_batch = y_train[batch_indices]
        
        # Compute loss and gradients
        if loss_function == 'svm':
            loss, dW, db = classifier.svm_loss(X_batch, y_batch, reg)
        else:  # softmax
            loss, dW, db = classifier.softmax_loss(X_batch, y_batch, reg)
        
        loss_history.append(loss)
        
        # Update parameters
        classifier.W -= learning_rate * dW
        classifier.b -= learning_rate * db
        
        # Check accuracy periodically
        if it % 100 == 0:
            train_acc = classifier.accuracy(X_train[:1000], y_train[:1000])
            val_acc = classifier.accuracy(X_val, y_val)
            train_acc_history.append(train_acc)
            val_acc_history.append(val_acc)
            
            if verbose:
                print(f"Iter {it:4d}/{num_iters}: Loss={loss:.4f}, Train Acc={train_acc:.2%}, Val Acc={val_acc:.2%}")
    
    return {
        'loss_history': loss_history,
        'train_acc_history': train_acc_history,
        'val_acc_history': val_acc_history
    }

# Train Softmax classifier
print("Training Softmax Classifier...\n")
softmax_clf = LinearClassifier()
softmax_history = train_linear_classifier(
    softmax_clf, X_train_flat, y_train, X_val_flat, y_val,
    loss_function='softmax',
    learning_rate=1e-3,
    reg=1e-5,
    num_iters=1000)

# Train SVM classifier for comparison
print("\nTraining SVM Classifier...\n")
svm_clf = LinearClassifier()
svm_history = train_linear_classifier(
    svm_clf, X_train_flat, y_train, X_val_flat, y_val,
    loss_function='svm',
    learning_rate=1e-3,
    reg=1e-5,
    num_iters=1000)

# Visualize training
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
# Plot 1: Loss curves
axes[0].plot(softmax_history['loss_history'], label='Softmax', alpha=0.7)
axes[0].plot(svm_history['loss_history'], label='SVM', alpha=0.7)
axes[0].set_xlabel('Iteration', fontsize=11)
axes[0].set_ylabel('Loss', fontsize=11)
axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Accuracy curves
iters_check = np.arange(0, 1000, 100)
axes[1].plot(iters_check, softmax_history['val_acc_history'], 'o-', label='Softmax', linewidth=2)
axes[1].plot(iters_check, svm_history['val_acc_history'], 's-', label='SVM', linewidth=2)
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Validation Accuracy', fontsize=11)
axes[1].set_title('Validation Accuracy', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# Plot 3: Visualize learned weights (as images)
W_img = softmax_clf.W.T.reshape(10, 32, 32, 3)  # (10, 32, 32, 3)
W_grid = np.zeros((32*2, 32*5, 3))
for i in range(10):
    row, col = i // 5, i % 5
    W_normalized = (W_img[i] - W_img[i].min()) / (W_img[i].max() - W_img[i].min() + 1e-10)
    W_grid[row*32:(row+1)*32, col*32:(col+1)*32] = W_normalized

axes[2].imshow(W_grid)
axes[2].set_title('Learned Weight Templates', fontsize=12, fontweight='bold')
axes[2].axis('off')
plt.tight_layout()
plt.show()

# Final test accuracy
test_acc_softmax = softmax_clf.accuracy(X_test_flat, y_test)
test_acc_svm = svm_clf.accuracy(X_test_flat, y_test)
print(f"\n" + "="*50)
print("Final Test Accuracy:")
print(f"  Softmax: {test_acc_softmax:.2%}")
print(f"  SVM:     {test_acc_svm:.2%}")
print(f"  kNN:     {max(max(accuracies_l1), max(accuracies_l2)):.2%} (baseline)")
print("="*50)

print("\nðŸ”‘ Key insights:")
print("   â€¢ Linear classifier: f(x) = Wx + b (one template per class)")
print("   â€¢ SVM: Margin-based (hinge loss)")
print("   â€¢ Softmax: Probability-based (cross-entropy)")
print("   â€¢ Both outperform kNN and train fast!")
print("   â€¢ Weights look like averaged class templates")
print("\nâœ“ Linear classifiers complete! Let's add nonlinearity...")

# Section 4: Optimization - SGD, Momentum, and Learning Rate Schedules
# Stochastic Gradient Descent (SGD)
# Update rule:
# where is the learning rate.
# SGD with Momentum
# Add velocity term:
# where is the momentum coefficient (typically 0.9).
# Benefit: Smooths updates, accelerates through ravines.
# Learning Rate Schedules
# Step decay:
# Exponential decay:
# 1/t decay:
# Babysitting the Learning Process
# CS231n wisdom:
# Start with small lr (1e-3 to 1e-4)
# Monitor loss: Should decrease smoothly
# Check gradients: Not too small, not too large
# Visualize weights: Should show structure
# Overfit small dataset first (sanity check)

class Optimizer:
    """Base optimizer class."""
    
    def __init__(self, learning_rate: float = 1e-3):
        self.learning_rate = learning_rate
    
    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:
        """Update parameter using gradient."""
        raise NotImplementedError

class SGD(Optimizer):
    """Vanilla SGD optimizer."""
    
    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:
        return param - self.learning_rate * grad

class SGDMomentum(Optimizer):
    """SGD with momentum."""
    
    def __init__(self, learning_rate: float = 1e-3, momentum: float = 0.9):
        super().__init__(learning_rate)
        self.momentum = momentum
        self.velocity = {}
    
    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:
        if param_id not in self.velocity:
            self.velocity[param_id] = np.zeros_like(param)
        
        self.velocity[param_id] = self.momentum * self.velocity[param_id] - self.learning_rate * grad
        return param + self.velocity[param_id]

class Adam(Optimizer):
    """Adam optimizer (adaptive learning rates)."""
    
    def __init__(self, learning_rate: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999):
        super().__init__(learning_rate)
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = 1e-8
        self.m = {}  # First moment
        self.v = {}  # Second moment
        self.t = {}  # Timestep
    
    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:
        if param_id not in self.m:
            self.m[param_id] = np.zeros_like(param)
            self.v[param_id] = np.zeros_like(param)
            self.t[param_id] = 0
        
        self.t[param_id] += 1
        t = self.t[param_id]
        
        # Update biased moments
        self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grad
        self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (grad ** 2)
        
        # Bias correction
        m_hat = self.m[param_id] / (1 - self.beta1 ** t)
        v_hat = self.v[param_id] / (1 - self.beta2 ** t)
        
        # Update
        return param - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)

def learning_rate_schedule(initial_lr: float, iteration: int, schedule_type: str = 'step') -> float:
    """Compute learning rate with schedule.
        Args:
        initial_lr: Initial learning rate
        iteration: Current iteration
        schedule_type: 'step', 'exp', or 'inverse'
    """
    if schedule_type == 'step':
        # Decay by 0.5 every 250 iterations
        return initial_lr * (0.5 ** (iteration // 250))
    elif schedule_type == 'exp':
        # Exponential decay
        return initial_lr * np.exp(-0.001 * iteration)
    else:  # inverse
        # 1/t decay
        return initial_lr / (1 + 0.001 * iteration)

# Compare optimizers
print("Comparing optimizers...\n")
optimizers = {
    'SGD': SGD(learning_rate=1e-3),
    'SGD+Momentum': SGDMomentum(learning_rate=1e-3, momentum=0.9),
    'Adam': Adam(learning_rate=1e-3)
}

histories = {}
for name, optimizer in optimizers.items():
    print(f"Training with {name}...")
    clf = LinearClassifier()
    
    loss_history = []
    for it in range(500):
        batch_indices = np.random.choice(len(X_train_flat), 200)
        X_batch = X_train_flat[batch_indices]
        y_batch = y_train[batch_indices]
        
        loss, dW, db = clf.softmax_loss(X_batch, y_batch, reg=1e-5)
        loss_history.append(loss)
        
        if isinstance(optimizer, (SGDMomentum, Adam)):
            clf.W = optimizer.update(clf.W, dW, 'W')
            clf.b = optimizer.update(clf.b, db, 'b')
        else:
            clf.W = optimizer.update(clf.W, dW)
            clf.b = optimizer.update(clf.b, db)
    
    histories[name] = loss_history
    final_acc = clf.accuracy(X_val_flat, y_val)
    print(f"  Final val acc: {final_acc:.2%}\n")

# Visualize optimizer comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
# Plot 1: Loss curves
for name, history in histories.items():
    axes[0].plot(history, label=name, linewidth=2, alpha=0.8)
axes[0].set_xlabel('Iteration', fontsize=11)
axes[0].set_ylabel('Loss', fontsize=11)
axes[0].set_title('Optimizer Comparison', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].set_yscale('log')

# Plot 2: Learning rate schedules
iters = np.arange(1000)
for schedule in ['step', 'exp', 'inverse']:
    lrs = [learning_rate_schedule(1e-3, it, schedule) for it in iters]
    axes[1].plot(iters, lrs, label=schedule.capitalize(), linewidth=2)
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Learning Rate', fontsize=11)
axes[1].set_title('Learning Rate Schedules', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_yscale('log')

plt.tight_layout()
plt.show()

print("\nðŸ”‘ Key insights:")
print("   â€¢ SGD: Simple but can be slow")
print("   â€¢ Momentum: Smooths updates, accelerates convergence")
print("   â€¢ Adam: Adaptive rates, often works out-of-the-box")
print("   â€¢ Learning rate schedule: Helps fine-tuning")
print("   â€¢ Babysitting: Monitor loss, check gradients, visualize weights")
print("\nâœ“ Optimization complete!")

# Section 5: Neural Networks - Adding Nonlinearity
# Linear classifiers have fundamental limits. Neural networks add nonlinearity through hidden layers.
# 2-Layer Neural Network
# where:
# : Input (3072)
# : First layer weights
# : Hidden layer (e.g., H=100)
# : Second layer weights
# : Output scores (10)
# Activation Functions
# ReLU (Rectified Linear Unit):
# Sigmoid:
# Tanh:
# ReLU is preferred: Fast, no saturation, works well in practice.
# Backpropagation
# Chain rule through computational graph:
# Forward pass: Compute activations
# Backward pass: Compute gradients
# For ReLU:

class TwoLayerNet:
    """Two-layer fully-connected neural network."""
    
    def __init__(self, input_dim: int = 3072, hidden_dim: int = 100, num_classes: int = 10):
        """Initialize network with Xavier/He initialization."""
        self.params = {}
        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)
        self.params['b1'] = np.zeros(hidden_dim)
        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * np.sqrt(2.0 / hidden_dim)
        self.params['b2'] = np.zeros(num_classes)
    
    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Forward pass with caching for backprop.
                Returns:
            scores: (N, C) class scores
            cache: Dictionary with intermediate values
        """
        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']
        
        # Layer 1: Linear + ReLU
        z1 = X @ W1 + b1  # (N, H)
        h1 = np.maximum(0, z1)  # ReLU
        
        # Layer 2: Linear
        scores = h1 @ W2 + b2  # (N, C)
        
        cache = {'X': X, 'z1': z1, 'h1': h1}
        return scores, cache
    
    def loss(self, X: np.ndarray, y: np.ndarray, reg: float = 0.0) -> Tuple[float, Dict]:
        """Compute loss and gradients.
                Returns:
            loss: Scalar loss
            grads: Dictionary with gradients for each parameter
        """
        N = X.shape[0]
        
        # Forward pass
        scores, cache = self.forward(X)
        
        # Compute softmax loss
        scores -= np.max(scores, axis=1, keepdims=True)  # Numerical stability
        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
        
        loss = -np.sum(np.log(probs[range(N), y] + 1e-10)) / N
        loss += reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))
        
        # Backward pass
        grads = {}
        
        # Gradient on scores
        dscores = probs.copy()
        dscores[range(N), y] -= 1
        dscores /= N
        
        # Layer 2 gradients
        grads['W2'] = cache['h1'].T @ dscores + 2 * reg * self.params['W2']
        grads['b2'] = np.sum(dscores, axis=0)
        
        # Backprop to hidden layer
        dh1 = dscores @ self.params['W2'].T
        
        # ReLU backward
        dz1 = dh1 * (cache['z1'] > 0)  # ReLU derivative
        
        # Layer 1 gradients
        grads['W1'] = cache['X'].T @ dz1 + 2 * reg * self.params['W1']
        grads['b1'] = np.sum(dz1, axis=0)
        
        return loss, grads
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predict class labels."""
        scores, _ = self.forward(X)
        return np.argmax(scores, axis=1)
    
    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:
        """Compute accuracy."""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

def train_neural_network(net: TwoLayerNet,
                        X_train: np.ndarray,
                        y_train: np.ndarray,
                        X_val: np.ndarray,
                        y_val: np.ndarray,
                        learning_rate: float = 1e-3,
                        reg: float = 1e-5,
                        num_iters: int = 2000,
                        batch_size: int = 200,
                        verbose: bool = True) -> Dict:
    """Train neural network using SGD with momentum."""
    N = X_train.shape[0]
    loss_history = []
    train_acc_history = []
    val_acc_history = []
    
    # Use momentum
    velocity = {key: np.zeros_like(val) for key, val in net.params.items()}
    momentum = 0.9
    
    for it in range(num_iters):
        # Sample mini-batch
        batch_indices = np.random.choice(N, batch_size)
        X_batch = X_train[batch_indices]
        y_batch = y_train[batch_indices]
        
        # Compute loss and gradients
        loss, grads = net.loss(X_batch, y_batch, reg)
        loss_history.append(loss)
        
        # Update with momentum
        for param_name in net.params:
            velocity[param_name] = momentum * velocity[param_name] - learning_rate * grads[param_name]
            net.params[param_name] += velocity[param_name]
        
        # Check accuracy periodically
        if it % 200 == 0:
            train_acc = net.accuracy(X_train[:1000], y_train[:1000])
            val_acc = net.accuracy(X_val, y_val)
            train_acc_history.append(train_acc)
            val_acc_history.append(val_acc)
            
            if verbose:
                print(f"Iter {it:4d}: Loss={loss:.4f}, Train={train_acc:.2%}, Val={val_acc:.2%}")
    
    return {
        'loss_history': loss_history,
        'train_acc_history': train_acc_history,
        'val_acc_history': val_acc_history
    }

# Train neural network
print("Training 2-Layer Neural Network...\n")
net = TwoLayerNet(input_dim=3072, hidden_dim=100, num_classes=10)
nn_history = train_neural_network(
    net, X_train_flat, y_train, X_val_flat, y_val,
    learning_rate=1e-3,
    reg=1e-5,
    num_iters=2000)

# Visualize training
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
# Plot 1: Loss curve
axes[0].plot(nn_history['loss_history'], linewidth=2, color='darkblue')
axes[0].set_xlabel('Iteration', fontsize=11)
axes[0].set_ylabel('Loss', fontsize=11)
axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Plot 2: Accuracy curves
iters_check = np.arange(0, 2000, 200)
axes[1].plot(iters_check, nn_history['train_acc_history'], 'o-', label='Train', linewidth=2)
axes[1].plot(iters_check, nn_history['val_acc_history'], 's-', label='Validation', linewidth=2)
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Accuracy', fontsize=11)
axes[1].set_title('Train vs Validation Accuracy', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# Plot 3: Visualize first layer weights
W1 = net.params['W1'].T  # (H, D)
W1_img = W1[:64].reshape(64, 32, 32, 3)  # First 64 neurons
W1_grid = np.zeros((32*8, 32*8, 3))
for i in range(64):
    row, col = i // 8, i % 8
    w = W1_img[i]
    w_norm = (w - w.min()) / (w.max() - w.min() + 1e-10)
    W1_grid[row*32:(row+1)*32, col*32:(col+1)*32] = w_norm

axes[2].imshow(W1_grid)
axes[2].set_title('First Layer Weights (Filters)', fontsize=12, fontweight='bold')
axes[2].axis('off')
plt.tight_layout()
plt.show()

# Test accuracy
test_acc_nn = net.accuracy(X_test_flat, y_test)
print(f"\n" + "="*50)
print("Test Accuracy Comparison:")
print(f"  Neural Network: {test_acc_nn:.2%}")
print(f"  Softmax:        {test_acc_softmax:.2%}")
print(f"  kNN:            {max(max(accuracies_l1), max(accuracies_l2)):.2%}")
print("="*50)

print("\nðŸ”‘ Key insights:")
print("   â€¢ Nonlinearity (ReLU) enables learning complex functions")
print("   â€¢ Hidden layer learns features, output layer classifies")
print("   â€¢ Neural network >> linear classifier!")
print("   â€¢ First layer weights look like edge/color detectors")
print("   â€¢ More layers = more capacity (but also harder to train)")
print("\nâœ“ Neural networks complete! Now let's add conv layers...")

# Section 6: Convolutional Neural Networks (CNNs)
# Key insight: Images have spatial structure! Fully-connected layers ignore this.
# Convolutional Layer
# Apply filters (kernels) to local regions:
# Parameters:
# Filter size: (typically 3Ã—3 or 5Ã—5)
# Stride: How much to move filter (typically 1 or 2)
# Padding: Add zeros around border to preserve size
# Output size:
# where = padding, = stride.
# Max Pooling
# Downsample by taking maximum in each region:
# Benefits:
# Reduces spatial size
# Translation invariance
# Controls overfitting
# Why CNNs Work
# Parameter sharing: Same filter applied everywhere (much fewer params than FC)
# Local connectivity: Each neuron only looks at local patch
# Translation invariance: Same features everywhere in image
# Hierarchical features: Early layers = edges, late layers = objects

def conv2d_forward(X: np.ndarray, W: np.ndarray, b: np.ndarray, 
                   stride: int = 1, pad: int = 0) -> Tuple[np.ndarray, Dict]:
    """Forward pass for convolutional layer.
        Args:
        X: (N, C_in, H, W) input
        W: (C_out, C_in, K, K) filters
        b: (C_out,) biases
        stride: Stride
        pad: Padding
        Returns:
        out: (N, C_out, H_out, W_out) output
        cache: Tuple for backprop
    """
    N, C_in, H, W = X.shape
    C_out, _, K, _ = W.shape
    
    # Add padding
    X_pad = np.pad(X, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')
    
    # Output dimensions
    H_out = (H + 2*pad - K) // stride + 1
    W_out = (W + 2*pad - K) // stride + 1
    
    # Initialize output
    out = np.zeros((N, C_out, H_out, W_out))
    
    # Naive implementation (loop-based, slow but clear)
    for i in range(H_out):
        for j in range(W_out):
            h_start = i * stride
            h_end = h_start + K
            w_start = j * stride
            w_end = w_start + K
            
            # Extract patch
            X_patch = X_pad[:, :, h_start:h_end, w_start:w_end]  # (N, C_in, K, K)
            
            # Convolve each filter
            for c in range(C_out):
                out[:, c, i, j] = np.sum(X_patch * W[c], axis=(1, 2, 3)) + b[c]
    
    cache = (X, W, b, stride, pad)
    return out, cache

def maxpool2d_forward(X: np.ndarray, pool_size: int = 2, stride: int = 2) -> Tuple[np.ndarray, Dict]:
    """Forward pass for max pooling layer.
        Args:
        X: (N, C, H, W) input
        pool_size: Size of pooling window
        stride: Stride
        Returns:
        out: (N, C, H_out, W_out) output
        cache: Tuple for backprop
    """
    N, C, H, W = X.shape
    
    H_out = (H - pool_size) // stride + 1
    W_out = (W - pool_size) // stride + 1
    
    out = np.zeros((N, C, H_out, W_out))
    
    for i in range(H_out):
        for j in range(W_out):
            h_start = i * stride
            h_end = h_start + pool_size
            w_start = j * stride
            w_end = w_start + pool_size
            
            # Max over spatial window
            X_patch = X[:, :, h_start:h_end, w_start:w_end]
            out[:, :, i, j] = np.max(X_patch, axis=(2, 3))
    
    cache = (X, pool_size, stride)
    return out, cache

def relu_forward(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Forward pass for ReLU."""
    out = np.maximum(0, X)
    cache = X
    return out, cache

# Test CNN layers
print("Testing CNN layers...\n")
# Test convolutional layer
X_test = X_train[:10].transpose(0, 3, 1, 2)  # (N, C, H, W)
W_test = np.random.randn(16, 3, 5, 5) * 0.01  # 16 filters, 5Ã—5, 3 channels
b_test = np.zeros(16)
out_conv, _ = conv2d_forward(X_test, W_test, b_test, stride=1, pad=2)
print(f"Conv layer: Input {X_test.shape} â†’ Output {out_conv.shape}")

# Test max pooling
out_pool, _ = maxpool2d_forward(out_conv, pool_size=2, stride=2)
print(f"Max pool:   Input {out_conv.shape} â†’ Output {out_pool.shape}")

# Test ReLU
out_relu, _ = relu_forward(out_pool)
print(f"ReLU:       Input {out_pool.shape} â†’ Output {out_relu.shape}")

# Visualize learned filters (example)
fig, axes = plt.subplots(4, 4, figsize=(10, 10))
for i in range(16):
    ax = axes[i // 4, i % 4]
    # Visualize filter (normalize each channel separately)
    filt = W_test[i].transpose(1, 2, 0)  # (K, K, 3)
    filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)
    ax.imshow(filt_norm)
    ax.set_title(f'Filter {i}')
    ax.axis('off')
plt.suptitle('Random Conv Filters (5Ã—5, 3 channels)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nðŸ”‘ Key insights:")
print("   â€¢ Conv layer: Apply filters to local regions")
print("   â€¢ Parameter sharing: Same filter everywhere (far fewer params)")
print("   â€¢ Max pooling: Downsample, translation invariance")
print("   â€¢ ReLU: Nonlinearity, fast and effective")
print("   â€¢ Stacking: Conv â†’ ReLU â†’ Pool â†’ repeat")
print("\nâœ“ CNN layers complete!")

# Section 7: Complete CNN Architecture - Mini AlexNet
# Let's build a simplified AlexNet for our 32Ã—32 images.
# AlexNet Architecture (Simplified)
# Input: 32Ã—32Ã—3
# â†“
# Conv1: 32 filters, 5Ã—5, stride 1, pad 2 â†’ 32Ã—32Ã—32
# ReLU â†’ MaxPool (2Ã—2, stride 2) â†’ 16Ã—16Ã—32
# â†“
# Conv2: 64 filters, 3Ã—3, stride 1, pad 1 â†’ 16Ã—16Ã—64
# ReLU â†’ MaxPool (2Ã—2, stride 2) â†’ 8Ã—8Ã—64
# â†“
# Flatten â†’ 4096
# â†“
# FC1: 4096 â†’ 256
# ReLU
# â†“
# FC2: 256 â†’ 10
# Softmax
# Parameter Count
# Conv layers:
# Conv1: 32 Ã— (5Ã—5Ã—3 + 1) = 2,432
# Conv2: 64 Ã— (3Ã—3Ã—32 + 1) = 18,496
# FC layers:
# FC1: 4096 Ã— 256 = 1,048,576
# FC2: 256 Ã— 10 = 2,560
# Total: ~1.07M parameters (vs 30M for pure FC!)
# Insight: CNNs are much more parameter-efficient than FC networks for images!

class SimpleCNN:
    """Simple CNN for image classification (toy AlexNet)."""
    
    def __init__(self):
        """Initialize with He initialization."""
        self.params = {}
        
        # Conv1: 3 â†’ 32, 5Ã—5
        self.params['W1'] = np.random.randn(32, 3, 5, 5) * np.sqrt(2.0 / (3*5*5))
        self.params['b1'] = np.zeros(32)
        
        # Conv2: 32 â†’ 64, 3Ã—3
        self.params['W2'] = np.random.randn(64, 32, 3, 3) * np.sqrt(2.0 / (32*3*3))
        self.params['b2'] = np.zeros(64)
        
        # FC1: 4096 â†’ 256
        self.params['W3'] = np.random.randn(4096, 256) * np.sqrt(2.0 / 4096)
        self.params['b3'] = np.zeros(256)
        
        # FC2: 256 â†’ 10
        self.params['W4'] = np.random.randn(256, 10) * np.sqrt(2.0 / 256)
        self.params['b4'] = np.zeros(10)
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """Forward pass (inference mode, simplified).
                Args:
            X: (N, H, W, C) input images
                Returns:
            scores: (N, 10) class scores
        """
        # Convert to (N, C, H, W) for conv layers
        X = X.transpose(0, 3, 1, 2)
        
        # Conv1 â†’ ReLU â†’ Pool
        out, _ = conv2d_forward(X, self.params['W1'], self.params['b1'], stride=1, pad=2)
        out, _ = relu_forward(out)
        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)
        
        # Conv2 â†’ ReLU â†’ Pool
        out, _ = conv2d_forward(out, self.params['W2'], self.params['b2'], stride=1, pad=1)
        out, _ = relu_forward(out)
        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)
        
        # Flatten
        N = out.shape[0]
        out = out.reshape(N, -1)  # (N, 4096)
        
        # FC1 â†’ ReLU
        out = out @ self.params['W3'] + self.params['b3']
        out = np.maximum(0, out)
        
        # FC2
        scores = out @ self.params['W4'] + self.params['b4']
        
        return scores
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predict class labels."""
        scores = self.forward(X)
        return np.argmax(scores, axis=1)
    
    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:
        """Compute accuracy."""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

# Test CNN (note: full training would be slow in pure NumPy, so we'll test architecture)
print("Building SimpleCNN (toy AlexNet)...\n")
cnn = SimpleCNN()

# Count parameters
total_params = sum(p.size for p in cnn.params.values())
print(f"Total parameters: {total_params:,}")

# Test forward pass
X_sample = X_train[:5]
scores = cnn.forward(X_sample)
print(f"\nForward pass test:")
print(f"  Input shape:  {X_sample.shape}")
print(f"  Output shape: {scores.shape}")
print(f"  Predictions:  {cnn.predict(X_sample)}")

# Random initialization accuracy
random_acc = cnn.accuracy(X_val[:100], y_val[:100])
print(f"\nRandom initialization accuracy: {random_acc:.2%} (expected ~10% for 10 classes)")

print("\n" + "="*70)
print("CNN Architecture Summary")
print("="*70)
print(f"Layer 1: Conv (3â†’32, 5Ã—5) + ReLU + MaxPool  â†’  16Ã—16Ã—32")
print(f"Layer 2: Conv (32â†’64, 3Ã—3) + ReLU + MaxPool â†’  8Ã—8Ã—64")
print(f"Layer 3: Flatten                             â†’  4096")
print(f"Layer 4: FC (4096â†’256) + ReLU                â†’  256")
print(f"Layer 5: FC (256â†’10)                         â†’  10")
print(f"\nTotal parameters: {total_params:,}")
print(f"Equivalent FC network: ~30,000,000 parameters (30Ã— more!)")
print("="*70)

print("\nðŸ”‘ Key insights:")
print("   â€¢ CNNs: Stack Conv+ReLU+Pool, then FC layers")
print("   â€¢ Parameter efficiency: 1M params vs 30M for FC")
print("   â€¢ Spatial hierarchy: Early = edges, Late = objects")
print("   â€¢ AlexNet (2012): First ImageNet breakthrough with CNNs")
print("   â€¢ Modern CNNs: ResNet, EfficientNet, etc. (same principles!)")
print("\nâœ“ CNN architecture complete!")

# Section 8: Visualization, Saliency Maps, and Transfer Learning
# Visualization Techniques
# 1. Filter Visualization
# Show what first-layer filters look like
# Early layers: edges, colors, textures
# 2. Activation Maps
# Show which neurons activate for given input
# See what features network detects
# 3. Saliency Maps
# Compute gradient of output w.r.t. input: 
# Shows which pixels matter most for prediction
# 4. Class Visualization
# Generate image that maximizes class score
# Reveals what network thinks each class looks like
# Transfer Learning
# Key insight: Features from ImageNet transfer to other tasks!
# Strategy:
# Pre-train on large dataset (ImageNet)
# Replace final layer for new task
# Fine-tune on small dataset
# Why it works: Early layers learn universal features (edges, textures).

def compute_saliency_map(net: SimpleCNN, X: np.ndarray, y: int) -> np.ndarray:
    """Compute saliency map for a single image.
        Args:
        net: Trained network
        X: (H, W, C) single image
        y: Target class
        Returns:
        saliency: (H, W) saliency map
    """
    X = X[np.newaxis, ...]  # Add batch dimension
    
    # Forward pass
    scores = net.forward(X)
    
    # Approximate gradient using finite differences
    # (Full backprop implementation omitted for brevity)
    eps = 1e-5
    saliency = np.zeros((32, 32))
    
    # Sample-based approximation (for speed)
    for i in range(0, 32, 4):
        for j in range(0, 32, 4):
            # Perturb pixel
            X_perturb = X.copy()
            X_perturb[0, i, j, :] += eps
            
            # Compute score change
            scores_perturb = net.forward(X_perturb)
            grad_approx = (scores_perturb[0, y] - scores[0, y]) / eps
            saliency[i:i+4, j:j+4] = abs(grad_approx)
    
    return saliency

def visualize_filters_and_activations(cnn: SimpleCNN, X_sample: np.ndarray):
    """Visualize learned filters and activation maps."""
    fig, axes = plt.subplots(3, 4, figsize=(15, 12))
    
    # Row 1: Input images
    for i in range(4):
        axes[0, i].imshow(X_sample[i])
        axes[0, i].set_title(f'Input {i}')
        axes[0, i].axis('off')
    
    # Row 2: First layer filters (sample)
    W1 = cnn.params['W1']  # (32, 3, 5, 5)
    for i in range(4):
        filt = W1[i].transpose(1, 2, 0)  # (5, 5, 3)
        filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)
        axes[1, i].imshow(filt_norm)
        axes[1, i].set_title(f'Filter {i}')
        axes[1, i].axis('off')
    
    # Row 3: Saliency maps
    for i in range(4):
        y_pred = cnn.predict(X_sample[i:i+1])[0]
        saliency = compute_saliency_map(cnn, X_sample[i], y_pred)
        axes[2, i].imshow(saliency, cmap='hot')
        axes[2, i].set_title(f'Saliency (pred={y_pred})')
        axes[2, i].axis('off')
    
    plt.suptitle('CNN Visualization: Filters, Activations, Saliency', 
                fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Visualize CNN
print("Visualizing CNN components...\n")
visualize_filters_and_activations(cnn, X_val[:4])

print("\nðŸ”‘ Key insights from visualization:")
print("   â€¢ First layer filters: Learn edge/color/texture detectors")
print("   â€¢ Saliency maps: Show which pixels matter for prediction")
print("   â€¢ Activation maps: Reveal what features network detects")
print("   â€¢ Class visualization: Generate prototypical examples")
print("\nðŸŽ“ Transfer Learning Strategy:")
print("   1. Pre-train on ImageNet (millions of images)")
print("   2. Keep conv layers (feature extractor)")
print("   3. Replace FC layers for new task")
print("   4. Fine-tune on small target dataset")
print("   â†’ Works because early features are universal!")
print("\nâœ“ Visualization complete!")

# Section 9: Babysitting the Learning Process - Practical Tips
# CS231n's Wisdom for Training Neural Networks
# 1. Data Preprocessing
# Normalize: Mean 0, std 1
# Augmentation: Flips, crops, color jitter
# Whitening: Decorrelate features (PCA)
# 2. Weight Initialization
# Xavier: 
# for tanh
# He: 
# for ReLU
# Biases: Usually 0
# 3. Sanity Checks
# Overfit tiny dataset: Should get ~100% accuracy
# Check loss: Initial loss should match theorySoftmax with C classes: 
# Gradient check: Numerical vs analytical gradients
# 4. Hyperparameter Tuning
# Learning rate: Most important!Too high: Loss explodes
# Too low: No learning
# Sweet spot: Loss decreases steadily
# Regularization: Start with 1e-5, tune on validation
# Batch size: 32-256 typically
# 5. Monitoring Training
# Loss curves: Should decrease smoothly
# Train/val gap: Indicates overfitting
# Weight updates: ~1e-3 of weights per iteration
# Activation histograms: Check for dead neurons
# 6. Common Mistakes
# Forgot to normalize data
# Learning rate too high/low
# Regularization too strong
# Batch size too small (noisy gradients)
# Not using validation set properly

# Demonstrate babysitting tips
def sanity_check_loss(num_classes: int = 10) -> float:
    """Expected initial loss for softmax with random weights."""
    return -np.log(1.0 / num_classes)

def overfit_small_dataset(net: TwoLayerNet, X_small: np.ndarray, y_small: np.ndarray, num_iters: int = 500):
    """Sanity check: Should be able to overfit small dataset."""
    print("Sanity check: Overfitting 50 samples...")
    
    losses = []
    accs = []
    
    for it in range(num_iters):
        loss, grads = net.loss(X_small, y_small, reg=0)  # No regularization
        losses.append(loss)
        accs.append(net.accuracy(X_small, y_small))
        
        # Large learning rate for overfitting
        for param in net.params:
            net.params[param] -= 1e-2 * grads[param]
    
    return losses, accs

def plot_training_diagnostics(history: Dict):
    """Plot comprehensive training diagnostics."""
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # Plot 1: Loss curve (log scale)
    axes[0, 0].plot(history['loss_history'])
    axes[0, 0].set_xlabel('Iteration')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Loss (log scale)')
    axes[0, 0].set_yscale('log')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Plot 2: Train vs Val accuracy
    iters = np.arange(0, len(history['loss_history']), len(history['loss_history'])//len(history['train_acc_history']))
    axes[0, 1].plot(iters, history['train_acc_history'], label='Train')
    axes[0, 1].plot(iters, history['val_acc_history'], label='Val')
    axes[0, 1].set_xlabel('Iteration')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_title('Train/Val Accuracy Gap')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Plot 3: Learning rate schedule
    iters_range = np.arange(len(history['loss_history']))
    lrs = [learning_rate_schedule(1e-3, it, 'step') for it in iters_range]
    axes[0, 2].plot(iters_range, lrs)
    axes[0, 2].set_xlabel('Iteration')
    axes[0, 2].set_ylabel('Learning Rate')
    axes[0, 2].set_title('Learning Rate Schedule')
    axes[0, 2].set_yscale('log')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Plot 4: Loss histogram
    axes[1, 0].hist(history['loss_history'][100:], bins=50, edgecolor='black', alpha=0.7)
    axes[1, 0].set_xlabel('Loss')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Loss Distribution')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # Plot 5: Loss smoothness (gradient of loss)
    loss_grad = np.diff(history['loss_history'])
    axes[1, 1].plot(loss_grad, alpha=0.5)
    axes[1, 1].plot(np.convolve(loss_grad, np.ones(50)/50, mode='valid'), linewidth=2, label='Smoothed')
    axes[1, 1].set_xlabel('Iteration')
    axes[1, 1].set_ylabel('Loss Gradient')
    axes[1, 1].set_title('Loss Change Rate')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # Plot 6: Overfitting indicator
    train_val_gap = np.array(history['train_acc_history']) - np.array(history['val_acc_history'])
    axes[1, 2].plot(iters, train_val_gap, linewidth=2, color='red')
    axes[1, 2].axhline(0, color='black', linestyle='--', alpha=0.5)
    axes[1, 2].set_xlabel('Iteration')
    axes[1, 2].set_ylabel('Train - Val Accuracy')
    axes[1, 2].set_title('Overfitting Indicator')
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].fill_between(iters, 0, train_val_gap, where=(train_val_gap > 0), 
                            color='red', alpha=0.3, label='Overfitting')
    axes[1, 2].legend()
    
    plt.suptitle('Training Diagnostics: Babysitting the Learning Process', 
                fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Run sanity checks
print("=" * 70)
print("Babysitting Tips: Practical Training Checks")
print("=" * 70)

# Check 1: Expected initial loss
expected_loss = sanity_check_loss(10)
print(f"\n1. Expected initial loss (10 classes): {expected_loss:.4f}")
print(f"   (Random softmax: -log(1/10) = -log(0.1) â‰ˆ 2.303)")

# Check 2: Overfit small dataset
print("\n2. Sanity check: Overfitting 50 samples...")
small_net = TwoLayerNet()
X_small = X_train_flat[:50]
y_small = y_train[:50]
losses_overfit, accs_overfit = overfit_small_dataset(small_net, X_small, y_small)
print(f"   Initial accuracy: {accs_overfit[0]:.2%}")
print(f"   Final accuracy:   {accs_overfit[-1]:.2%}")
print(f"   âœ“ Can overfit! (Should reach ~100%)")

# Check 3: Plot diagnostics
print("\n3. Training diagnostics (using previous neural network training)...")
plot_training_diagnostics(nn_history)

print("\n" + "=" * 70)
print("CS231n Babysitting Checklist:")
print("=" * 70)
print("\nâœ“ 1. Data preprocessing: Normalize, augment")
print("âœ“ 2. Weight initialization: Xavier/He")
print("âœ“ 3. Sanity checks: Overfit small set, check initial loss")
print("âœ“ 4. Learning rate: Start with 1e-3, tune carefully")
print("âœ“ 5. Monitor: Loss curves, train/val gap, gradients")
print("âœ“ 6. Regularization: Start weak, increase if overfitting")
print("\nðŸ’¡ Rule of thumb: If loss doesn't decrease, check learning rate!")
print("\nâœ“ Babysitting complete!")

# Section 10: Modern Architectures and Beyond
# CS231n provides the foundation. Modern architectures build on these principles.
# VGG (2014)
# Key idea: Stack many small (3Ã—3) convs
# Deeper networks > wider networks
# Simple, uniform architecture
# ResNet (2015) - See Paper #10!
# Key idea: Skip connections
# (learn residual)
# Enables training 1000+ layer networks
# Solves degradation problem
# Modern Trends (2020s)
# Vision Transformers (ViT)
# Replace convs with self-attention
# Treat image as sequence of patches
# Scales better than CNNs
# EfficientNet
# Compound scaling: depth + width + resolution
# Neural architecture search
# SOTA with fewer params
# Diffusion Models
# Generative models (DALL-E, Stable Diffusion)
# Still use conv backbones!
# The Big Picture
# CS231n teaches timeless principles:
# Representation learning: Learn features, not hand-craft
# Hierarchical features: Low-level â†’ high-level
# Inductive biases: CNNs for images, RNNs for sequences
# Optimization: Gradients, backprop, SGD
# Regularization: Prevent overfitting
# These apply to all of deep learningâ€”not just vision!
# Sutskever 30 Connection
# CS231n ties together multiple papers:
# #7: AlexNet (CNNs for ImageNet)
# #10: ResNet (skip connections)
# #11: Dilated Convolutions (receptive fields)
# #13: Transformers (attention for vision)
# This notebook is your vision foundation!

# Final summary and comparison
print("="*70)
print("CS231n: Complete Computer Vision Pipeline")
print("="*70)

# Summary table
results_summary = {
    'Method': ['kNN', 'Linear (Softmax)', 'Neural Network (2-layer)', 'CNN (Mini-AlexNet)'],
    'Parameters': ['0 (memorize)', '~31K', '~1M', '~1M'],
    'Accuracy': [f"{max(max(accuracies_l1), max(accuracies_l2)):.1%}", 
                f"{test_acc_softmax:.1%}",
                f"{test_acc_nn:.1%}",
                "~60-70% (if trained)"],
    'Speed': ['Slow (test)', 'Fast', 'Fast', 'Medium'],
    'Key Insight': ['No training', 'One template per class', 'Nonlinear features', 'Spatial structure']
}

print("\nModel Comparison:")
print("-"*70)
for i in range(len(results_summary['Method'])):
    print(f"{results_summary['Method'][i]:25s} | "
          f"Params: {results_summary['Parameters'][i]:10s} | "
          f"Acc: {results_summary['Accuracy'][i]:10s}")
    print(f"{'':27s}   {results_summary['Key Insight'][i]}")
    print("-"*70)

print("\n" + "="*70)
print("Key Takeaways from CS231n")
print("="*70)
takeaways = [
    "1. IMAGE CLASSIFICATION PIPELINE",
    "   â€¢ Data â†’ Model â†’ Loss â†’ Optimization â†’ Prediction",
    "   â€¢ Each component matters!",
    "",
    "2. MODEL EVOLUTION",
    "   â€¢ kNN â†’ Linear â†’ NN â†’ CNN â†’ ResNet â†’ Transformers",
    "   â€¢ Each step adds capacity and inductive bias",
    "",
    "3. CONVOLUTIONAL NETWORKS",
    "   â€¢ Conv layers: Local connectivity, parameter sharing",
    "   â€¢ Pooling: Downsampling, invariance",
    "   â€¢ Hierarchy: Edges â†’ textures â†’ parts â†’ objects",
    "",
    "4. TRAINING TECHNIQUES",
    "   â€¢ SGD with momentum, learning rate schedules",
    "   â€¢ Xavier/He initialization",
    "   â€¢ Regularization: L2, dropout, data augmentation",
    "",
    "5. BABYSITTING NEURAL NETS",
    "   â€¢ Sanity checks: overfit small set, check initial loss",
    "   â€¢ Monitor: loss curves, train/val gap, gradients",
    "   â€¢ Hyperparameter tuning: learning rate is most important!",
    "",
    "6. VISUALIZATION",
    "   â€¢ Understand what network learns",
    "   â€¢ Filters, activations, saliency maps",
    "   â€¢ Debugging tool and interpretability",
    "",
    "7. TRANSFER LEARNING",
    "   â€¢ Pre-train on ImageNet, fine-tune on target task",
    "   â€¢ Early features are universal",
    "   â€¢ Enables learning from small datasets",
]

for line in takeaways:
    print(line)

print("\n" + "="*70)
print("Beyond CS231n: Modern Vision")
print("="*70)
print("\nâ€¢ ResNet (2015): Skip connections â†’ 1000+ layers")
print("â€¢ DenseNet (2016): Dense connections")
print("â€¢ EfficientNet (2019): NAS + compound scaling")
print("â€¢ Vision Transformers (2020): Attention for vision")
print("â€¢ ConvNeXt (2022): Modernized CNNs")
print("â€¢ Diffusion Models (2022): DALL-E, Stable Diffusion")
print("\nâ†’ All build on CS231n foundations!")
print("\n" + "="*70)
print("ðŸŽ“ CS231n: Complete! You've learned vision from first principles.")
print("="*70)
print("\nWhat you can do now:")
print("  âœ“ Understand how CNNs work (from scratch!)")
print("  âœ“ Train vision models (optimization, regularization)")
print("  âœ“ Debug neural networks (babysitting tips)")
print("  âœ“ Read modern papers (you have the foundation!)")
print("\nNext steps:")
print("  â†’ Implement in PyTorch for real datasets")
print("  â†’ Study ResNet (Paper #10 in this repo!)")
print("  â†’ Explore transformers (Paper #13)")
print("  â†’ Build your own vision systems!")
print("\nâœ¨ Welcome to computer vision! âœ¨")

```

```python [Torch]
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from typing import Tuple, List, Dict

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"CS231n Implementation using PyTorch (Running on {device})")

# -----------------------------------------------------------
# Section 1: Dataset - Synthetic CIFAR-10
# Reusing the Numpy data generation logic but converting to Tensors.
# -----------------------------------------------------------
def generate_synthetic_cifar_torch(num_samples: int = 1000, 
                                 img_size: int = 32) -> Tuple[torch.Tensor, torch.Tensor]:
    # Reuse Numpy generation logic for consistency
    # (Copying entire generation code here is redundant, so we wrap the Numpy function)
    # In a real scenario, this would be: transform = transforms.ToTensor(), dataset = CIFAR10(...)
    
    # Simple reuse of the logic provided in Numpy section
    X = np.zeros((num_samples, img_size, img_size, 3))
    y = np.random.randint(0, 10, num_samples)
    
    for i in range(num_samples):
        label = y[i]
        img = np.random.randn(img_size, img_size, 3) * 0.1
        # Simplified generation logic for brevity in Torch comparison
        if label < 3: img[16, 16, 0] = 1.0 # Point source
        elif label < 6: img[:, :, 0] = np.linspace(0, 1, 32) # Gradient
        else: img[10:20, 10:20, 1] = 1.0 # Box
        X[i] = np.clip(img, 0, 1)
        
    X_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2).to(device) # (N, C, H, W)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)
    return X_tensor, y_tensor

print("Generating synthetic data (Torch Tensor)...")
# Note: Torch uses (N, C, H, W) convention natively
X_train, y_train = generate_synthetic_cifar_torch(2000)
X_val, y_val = generate_synthetic_cifar_torch(400)
X_test, y_test = generate_synthetic_cifar_torch(400)

# Flatten for Linear/NN (N, D)
X_train_flat = X_train.flatten(start_dim=1)
X_val_flat = X_val.flatten(start_dim=1)
X_test_flat = X_test.flatten(start_dim=1)

# -----------------------------------------------------------
# Section 2: k-Nearest Neighbors (kNN)
# -----------------------------------------------------------
class KNearestNeighborTorch:
    """Efficient kNN using PyTorch broadcasting."""
    def __init__(self, k: int = 5):
        self.k = k
        self.X_train = None
        self.y_train = None
        
    def train(self, X: torch.Tensor, y: torch.Tensor):
        self.X_train = X
        self.y_train = y
        
    def predict(self, X: torch.Tensor, distance_metric: str = 'l2') -> torch.Tensor:
        # X: (N_test, D), X_train: (N_train, D)
        # Broadcasting: (N_test, 1, D) - (1, N_train, D)
        if distance_metric == 'l2':
            # cdist is highly optimized equivalent to sqrt(sum((x-y)^2))
            dists = torch.cdist(X, self.X_train, p=2) 
        else:
            dists = torch.cdist(X, self.X_train, p=1)
            
        # Topk returns largest, so we use negative distance or 'largest=False' (if supported)
        # Using topk(largest=False) is cleaner
        _, indices = dists.topk(self.k, largest=False)
        
        # Gather labels: (N_test, k)
        neighbor_labels = self.y_train[indices]
        
        # Mode (majority vote)
        y_pred, _ = torch.mode(neighbor_labels, dim=1)
        return y_pred

# -----------------------------------------------------------
# Section 3: Linear Classifier (Softmax)
# Mapped to: nn.Linear + CrossEntropyLoss
# -----------------------------------------------------------
class LinearClassifierTorch(nn.Module):
    def __init__(self, input_dim=3072, num_classes=10):
        super().__init__()
        # Corresponds to Numpy self.W and self.b
        self.linear = nn.Linear(input_dim, num_classes)
        # Initialize small random weights to match Numpy behavior
        nn.init.normal_(self.linear.weight, mean=0, std=0.0001)
        nn.init.zeros_(self.linear.bias)
        
    def forward(self, x):
        return self.linear(x)

# Training loop abstraction
def train_torch_model(model, X, y, epochs=1000, lr=1e-3, optimizer_cls=optim.SGD, **optim_kwargs):
    criterion = nn.CrossEntropyLoss() # Combines Softmax + LogLoss + Numerical Stability
    optimizer = optimizer_cls(model.parameters(), lr=lr, **optim_kwargs)
    
    losses = []
    
    for epoch in range(epochs):
        # Forward
        scores = model(X)
        loss = criterion(scores, y)
        
        # Backward (Autograd handles dW, db calculation)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if epoch % 100 == 0:
            acc = (scores.argmax(1) == y).float().mean()
            # print(f"Epoch {epoch}: Loss {loss.item():.4f}, Acc {acc:.2%}")
            
    return losses

print("Training Linear Classifier (Torch)...")
linear_model = LinearClassifierTorch().to(device)
linear_hist = train_torch_model(linear_model, X_train_flat, y_train, epochs=1000)

# -----------------------------------------------------------
# Section 5: Neural Network (2-Layer)
# Mapped to: nn.Sequential(Linear, ReLU, Linear)
# -----------------------------------------------------------
class TwoLayerNetTorch(nn.Module):
    def __init__(self, input_dim=3072, hidden_dim=100, num_classes=10):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),  # Corresponds to Numpy np.maximum(0, x)
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, x):
        return self.net(x)

print("Training 2-Layer NN (Torch)...")
nn_model = TwoLayerNetTorch().to(device)
# Using SGD with Momentum
nn_hist = train_torch_model(nn_model, X_train_flat, y_train, epochs=2000, 
                           lr=1e-3, optimizer_cls=optim.SGD, momentum=0.9)

# -----------------------------------------------------------
# Section 6 & 7: CNN (Mini AlexNet)
# Mapped to: nn.Conv2d, nn.MaxPool2d, nn.ReLU
# -----------------------------------------------------------
class SimpleCNNTorch(nn.Module):
    def __init__(self):
        super().__init__()
        # Conv1: 3 -> 32, 5x5, stride 1, pad 2
        # Corresponds to Numpy: conv2d_forward(..., pad=2)
        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Conv2: 32 -> 64, 3x3, stride 1, pad 1
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Flatten and FC layers
        # Input size calc: 32 -> 16 -> 8. Channel 64. Flat = 64*8*8 = 4096
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(4096, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
        
        # Initialize weights (He initialization)
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.fc(out)
        return out

print("Building SimpleCNN (Torch)...")
cnn_model = SimpleCNNTorch().to(device)
print(f"Total parameters: {sum(p.numel() for p in cnn_model.parameters()):,}")

# Test Forward Pass
with torch.no_grad():
    scores = cnn_model(X_train[:5])
    print(f"CNN Output shape: {scores.shape}")

# -----------------------------------------------------------
# Section 8: Saliency Maps (Autograd Magic)
# Mapped to: loss.backward() wrt Input
# -----------------------------------------------------------
def compute_saliency_torch(model, X, y):
    model.eval()
    X_var = X.clone().unsqueeze(0).requires_grad_(True) # (1, C, H, W)
    
    scores = model(X_var)
    score_target = scores[0, y]
    
    # Backward pass to get gradient wrt input image
    score_target.backward()
    
    # Saliency is the max absolute gradient across channels
    saliency, _ = torch.max(X_var.grad.abs(), dim=1)
    return saliency.squeeze().detach().cpu().numpy()

# Visualization
print("Computing Saliency Map (Torch)...")
saliency_map = compute_saliency_torch(cnn_model, X_train[0], y_train[0])

# Plotting Comparison
fig, ax = plt.subplots(1, 2, figsize=(10, 4))
ax[0].plot(nn_hist)
ax[0].set_title("Torch NN Training Loss")
ax[1].imshow(saliency_map, cmap='hot')
ax[1].set_title("Torch Saliency Map (Autograd)")
plt.show()

print("\nâœ“ Torch implementation complete! (Much more concise due to Autograd)")

```

:::

### å¯¹ç…§è®²è§£

1. **è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰ vs æ‰‹åŠ¨æŽ¨å¯¼**
* **Numpy**: ä½ å¿…é¡»æ˜¾å¼ç¼–å†™åå‘ä¼ æ’­é€»è¾‘ï¼ˆä¾‹å¦‚ `dscores = probs; dscores[y] -= 1`ï¼Œä»¥åŠ `dW = X.T @ dscores`ï¼‰ã€‚è¿™è¦æ±‚å¯¹çŸ©é˜µå¾®ç§¯åˆ†æžå…¶ç†Ÿæ‚‰ï¼Œä¸”æžæ˜“å‡ºé”™ã€‚
* **Torch**: æ ¸å¿ƒä»…ä»…æ˜¯ `loss.backward()`ã€‚PyTorch åŠ¨æ€æž„å»ºè®¡ç®—å›¾ï¼Œè‡ªåŠ¨è¿½è¸ªå¼ é‡æ“ä½œçš„æ¢¯åº¦ã€‚è¿™ä½¿å¾—å®žçŽ°æ›´æ·±çš„ç½‘ç»œï¼ˆå¦‚ ResNetï¼‰å˜å¾—è½»è€Œæ˜“ä¸¾ï¼Œæ— éœ€æ‰‹åŠ¨æŽ¨å¯¼å‡ åå±‚çš„é“¾å¼æ³•åˆ™ã€‚


2. **æ•°æ®æŽ’å¸ƒï¼ˆData Layoutï¼‰**
* **Numpy**: å›¾åƒå¤„ç†ä¸­å¸¸æ··ç”¨ `(N, H, W, C)`ï¼ˆå¯è§†åŒ–å‹å¥½ï¼‰å’Œ `(N, C, H, W)`ï¼ˆå·ç§¯è®¡ç®—å‹å¥½ï¼‰ã€‚åœ¨æä¾›çš„ Numpy ä»£ç ä¸­ï¼Œç»å¸¸éœ€è¦ `.transpose(0, 3, 1, 2)`ã€‚
* **Torch**: ä¸¥æ ¼çº¦å®šä½¿ç”¨ `(N, C, H, W)`ã€‚è¿™ä½¿å¾—ç®—å­ï¼ˆ`nn.Conv2d`ï¼‰å’Œç¡¬ä»¶åŠ é€Ÿï¼ˆcuDNNï¼‰èƒ½å¤Ÿç»Ÿä¸€å†…å­˜å¸ƒå±€ï¼Œæé«˜æ•ˆçŽ‡ã€‚


3. **ç®—å­æ•ˆçŽ‡**
* **Numpy**: `conv2d_forward` ä½¿ç”¨äº† 4 é‡ Python å¾ªçŽ¯ï¼ˆ`for i... for j...`ï¼‰ã€‚è¿™åœ¨è§£é‡ŠåŽŸç†æ—¶å¾ˆæ¸…æ™°ï¼Œä½†åœ¨å®žé™…è¿ç®—ä¸­æžæ…¢ã€‚
* **Torch**: `nn.Conv2d` åº•å±‚è°ƒç”¨ C++/CUDA å®žçŽ°çš„ `im2col` + `GEMM`ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰æˆ– Winograd ç®—æ³•ï¼Œé€Ÿåº¦é€šå¸¸æ˜¯çº¯ Python å¾ªçŽ¯çš„æ•°åƒå€ã€‚


4. **æ•°å€¼ç¨³å®šæ€§**
* **Numpy**: Softmax å®žçŽ°ä¸­éœ€è¦æ‰‹åŠ¨å‡åŽ»æœ€å¤§å€¼ `scores -= np.max(scores)` ä»¥é˜² `np.exp` æº¢å‡ºã€‚
* **Torch**: `nn.CrossEntropyLoss` å†…éƒ¨é›†æˆäº† `LogSoftmax` å’Œ `NLLLoss`ï¼Œå¹¶é€šè¿‡ Log-Sum-Exp æŠ€å·§è‡ªåŠ¨å¤„ç†äº†æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œç”¨æˆ·æ— éœ€æ“å¿ƒã€‚



```

<!-- AUTO_PDF_IMAGES_START -->

## è®ºæ–‡åŽŸå›¾ï¼ˆPDFï¼‰
> ä¸‹å›¾è‡ªåŠ¨æŠ½å–è‡ªåŽŸè®ºæ–‡ PDFï¼Œç”¨äºŽè¡¥å……æ¦‚å¿µã€ç»“æž„å’Œå®žéªŒç»†èŠ‚ã€‚
> æœªæ‰¾åˆ°å¯¹åº” PDFï¼Œå½“å‰æ–‡ç« æš‚ä¸æ’å…¥åŽŸå›¾ã€‚

<!-- AUTO_PDF_IMAGES_END -->

<!-- AUTO_INTERVIEW_QA_START -->

## é¢è¯•é¢˜ä¸Žç­”æ¡ˆ
> ä¸»é¢˜ï¼š**CS231n CNN**ï¼ˆå›´ç»• `è§†è§‰æ·±åº¦å­¦ä¹ åŸºç¡€`ï¼‰

### ä¸€ã€é€‰æ‹©é¢˜ï¼ˆ10é¢˜ï¼‰

1. åœ¨ CS231n CNN ä¸­ï¼Œæœ€å…³é”®çš„å»ºæ¨¡ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
   - A. è§†è§‰æ·±åº¦å­¦ä¹ åŸºç¡€
   - B. å·ç§¯
   - C. æ± åŒ–
   - D. åå‘ä¼ æ’­
   - **ç­”æ¡ˆï¼šA**

2. ä¸‹åˆ—å“ªä¸€é¡¹æœ€ç›´æŽ¥å¯¹åº” CS231n CNN çš„æ ¸å¿ƒæœºåˆ¶ï¼Ÿ
   - A. å·ç§¯
   - B. æ± åŒ–
   - C. åå‘ä¼ æ’­
   - D. ä¼˜åŒ–å™¨
   - **ç­”æ¡ˆï¼šB**

3. åœ¨å¤çŽ° CS231n CNN æ—¶ï¼Œä¼˜å…ˆè¦ä¿è¯å“ªé¡¹ä¸€è‡´æ€§ï¼Ÿ
   - A. åªçœ‹æœ€ç»ˆåˆ†æ•°
   - B. åªçœ‹è®­ç»ƒé›†è¡¨çŽ°
   - C. å®žçŽ°ä¸Žè®ºæ–‡è®¾ç½®å¯¹é½
   - D. å¿½ç•¥éšæœºç§å­
   - **ç­”æ¡ˆï¼šC**

4. å¯¹äºŽ CS231n CNNï¼Œå“ªä¸ªæŒ‡æ ‡æœ€èƒ½åæ˜ æ–¹æ³•æœ‰æ•ˆæ€§ï¼Ÿ
   - A. ä¸»æŒ‡æ ‡ä¸Žåˆ†ç»„æŒ‡æ ‡
   - B. åªçœ‹å•æ¬¡ç»“æžœ
   - C. åªçœ‹é€Ÿåº¦
   - D. åªçœ‹å‚æ•°é‡
   - **ç­”æ¡ˆï¼šA**

5. å½“ CS231n CNN æ¨¡åž‹å‡ºçŽ°æ•ˆæžœé€€åŒ–æ—¶ï¼Œé¦–è¦æ£€æŸ¥é¡¹æ˜¯ä»€ä¹ˆï¼Ÿ
   - A. æ•°æ®ä¸Žæ ‡ç­¾ç®¡çº¿
   - B. å…ˆå¢žå¤§æ¨¡åž‹åå€
   - C. éšæœºæ”¹æŸå¤±å‡½æ•°
   - D. åˆ é™¤éªŒè¯é›†
   - **ç­”æ¡ˆï¼šA**

6. CS231n CNN ä¸Žä¼ ç»Ÿ baseline çš„ä¸»è¦å·®å¼‚é€šå¸¸ä½“çŽ°åœ¨ï¼Ÿ
   - A. å½’çº³åç½®ä¸Žç»“æž„è®¾è®¡
   - B. ä»…å‚æ•°æ›´å¤š
   - C. ä»…è®­ç»ƒæ›´ä¹…
   - D. ä»…å­¦ä¹ çŽ‡æ›´å°
   - **ç­”æ¡ˆï¼šA**

7. è‹¥è¦æå‡ CS231n CNN çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ€ç¨³å¦¥çš„åšæ³•æ˜¯ï¼Ÿ
   - A. æ­£åˆ™åŒ–+æ¶ˆèžéªŒè¯
   - B. åªå †æ•°æ®ä¸å¤æ ¸
   - C. å…³é—­è¯„ä¼°è„šæœ¬
   - D. å–æ¶ˆå¯¹ç…§ç»„
   - **ç­”æ¡ˆï¼šA**

8. å…³äºŽ CS231n CNN çš„å®žéªŒè®¾è®¡ï¼Œä¸‹åˆ—è¯´æ³•æ›´åˆç†çš„æ˜¯ï¼Ÿ
   - A. å›ºå®šå˜é‡åšå¯å¤çŽ°å®žéªŒ
   - B. åŒæ—¶æ”¹åä¸ªè¶…å‚
   - C. åªå±•ç¤ºæœ€å¥½ä¸€æ¬¡
   - D. çœç•¥å¤±è´¥å®žéªŒ
   - **ç­”æ¡ˆï¼šA**

9. åœ¨å·¥ç¨‹éƒ¨ç½²ä¸­ï¼ŒCS231n CNN çš„å¸¸è§é£Žé™©æ˜¯ï¼Ÿ
   - A. æ•°å€¼ç¨³å®šä¸Žæ¼‚ç§»
   - B. åªå…³å¿ƒGPUåˆ©ç”¨çŽ‡
   - C. æ—¥å¿—è¶Šå°‘è¶Šå¥½
   - D. ä¸åšå›žå½’æµ‹è¯•
   - **ç­”æ¡ˆï¼šA**

10. å›žåˆ°è®ºæ–‡ä¸»å¼ ï¼ŒCS231n CNN æœ€ä¸åº”è¯¥è¢«è¯¯è§£ä¸ºï¼Ÿ
   - A. å¯æ›¿ä»£æ‰€æœ‰ä»»åŠ¡
   - B. æœ‰æ˜Žç¡®é€‚ç”¨è¾¹ç•Œ
   - C. ä¸éœ€è¦æ•°æ®è´¨é‡
   - D. ä¸éœ€è¦è¯¯å·®åˆ†æž
   - **ç­”æ¡ˆï¼šB**


### äºŒã€ä»£ç é¢˜ï¼ˆ10é¢˜ï¼Œå«å‚è€ƒç­”æ¡ˆï¼‰

1. å®žçŽ°ä¸€ä¸ªæœ€å°å¯è¿è¡Œçš„æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œè¾“å‡ºå¯ç”¨äºŽ CS231n CNN è®­ç»ƒçš„æ‰¹æ¬¡ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import numpy as np
     
     def make_batch(x, y, batch_size=32):
         idx = np.random.choice(len(x), batch_size, replace=False)
         return x[idx], y[idx]
     ```

2. å®žçŽ° CS231n CNN çš„æ ¸å¿ƒå‰å‘æ­¥éª¤ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼Œå¹¶è¿”å›žä¸­é—´å¼ é‡ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import numpy as np
     
     def forward_core(x, w, b):
         z = x @ w + b
         h = np.tanh(z)
         return h, {"z": z, "h": h}
     ```

3. å†™ä¸€ä¸ªè®­ç»ƒ stepï¼šå‰å‘ã€lossã€åå‘ã€æ›´æ–°ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     def train_step(model, optimizer, criterion, xb, yb):
         optimizer.zero_grad()
         pred = model(xb)
         loss = criterion(pred, yb)
         loss.backward()
         optimizer.step()
         return float(loss.item())
     ```

4. å®žçŽ°ä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼Œè¿”å›žä¸»æŒ‡æ ‡ä¸Žä¸€ä¸ªè¾…åŠ©æŒ‡æ ‡ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import numpy as np
     
     def evaluate(y_true, y_pred):
         acc = (y_true == y_pred).mean()
         err = 1.0 - acc
         return {"acc": float(acc), "err": float(err)}
     ```

5. å®žçŽ°æ¢¯åº¦è£å‰ªä¸Žå­¦ä¹ çŽ‡è°ƒåº¦çš„è®­ç»ƒå¾ªçŽ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import torch
     
     def train_loop(model, loader, optimizer, criterion, scheduler=None, clip=1.0):
         model.train()
         for xb, yb in loader:
             optimizer.zero_grad()
             loss = criterion(model(xb), yb)
             loss.backward()
             torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
             optimizer.step()
             if scheduler is not None:
                 scheduler.step()
     ```

6. å®žçŽ° ablation å¼€å…³ï¼šå¯åˆ‡æ¢æ˜¯å¦å¯ç”¨ `å·ç§¯`ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     def forward_with_ablation(x, module, use_feature=True):
         if use_feature:
             return module(x)
         return x
     ```

7. å®žçŽ°ä¸€ä¸ªé²æ£’çš„æ•°å€¼ç¨³å®š softmax / logsumexp å·¥å…·å‡½æ•°ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import numpy as np
     
     def stable_softmax(x, axis=-1):
         x = x - np.max(x, axis=axis, keepdims=True)
         ex = np.exp(x)
         return ex / np.sum(ex, axis=axis, keepdims=True)
     ```

8. å†™ä¸€ä¸ªå°åž‹å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ `æ± åŒ–` ç›¸å…³å¼ é‡å½¢çŠ¶æ­£ç¡®ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     def test_shape(out, expected_last_dim):
         assert out.ndim >= 2
         assert out.shape[-1] == expected_last_dim
     ```

9. å®žçŽ°æ¨¡åž‹æŽ¨ç†åŒ…è£…å™¨ï¼Œæ”¯æŒ batch è¾“å…¥å¹¶è¿”å›žç»“æž„åŒ–ç»“æžœã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     def infer(model, xb):
         logits = model(xb)
         pred = logits.argmax(dim=-1)
         return {"pred": pred, "logits": logits}
     ```

10. å®žçŽ°ä¸€ä¸ªå®žéªŒè®°å½•å™¨ï¼Œä¿å­˜è¶…å‚ã€æŒ‡æ ‡å’Œéšæœºç§å­ã€‚
   - å‚è€ƒç­”æ¡ˆï¼š
     ```python
     import json
     from pathlib import Path
     
     def save_run(path, cfg, metrics, seed):
         payload = {"cfg": cfg, "metrics": metrics, "seed": seed}
         Path(path).write_text(json.dumps(payload, ensure_ascii=False, indent=2))
     ```


<!-- AUTO_INTERVIEW_QA_END -->

